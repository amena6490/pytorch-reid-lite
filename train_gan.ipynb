{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#import math\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import cv2\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from IPython.display import HTML\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from nets.model_main import ft_net\n",
    "from utils import model_utils\n",
    "\n",
    "\n",
    "config = json.load(open(\"params.json\", \"r\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"parallels\"])[1:-1]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,2'\n",
    "print \"Currently using GPU\", str(config[\"parallels\"])[1:-1]\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpectralNorm\n",
    "https://github.com/christiancosgrove/pytorch-spectral-normalization-gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataloader for GAN\n",
    "- Every data iteration return in the format:  **{img, label, feature, mask}**\n",
    "\n",
    "- You need to extract the feature with provided model (for further usage, the features will be saved!)\n",
    "\n",
    "- You need to provide the file containing all the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic info \n",
    "\n",
    "config[\"batches_dir\"] = \"/world/data-gpu-94/sysu-reid/market_dataset\"\n",
    "label_dirs = [p for p in os.listdir(config[\"batches_dir\"])\n",
    "              if os.path.isdir(os.path.join(config[\"batches_dir\"], p))]\n",
    "config[\"num_labels\"] = len(label_dirs)\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "    \n",
    "def has_file_allowed_extension(filename, extensions):\n",
    "    \"\"\"Checks if a file is an allowed extension.\n",
    "\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(ext) for ext in extensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtions to build pytorch network and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_net(config, model_path, is_training=False):\n",
    "    net = ft_net(config,\n",
    "                 model_name=config[\"model_params\"][\"model\"],\n",
    "                 feature_dim=config[\"model_params\"].get(\"feature_dim\", 256),\n",
    "                 pcb_n_parts=config[\"model_params\"].get(\"pcb_n_parts\", 0),\n",
    "                 is_training=is_training)\n",
    "    net.train(is_training)\n",
    "    net = net.cuda()\n",
    "    if not is_training:\n",
    "        net.eval()  \n",
    "    \n",
    "    # Restore pretrain model\n",
    "    model_utils.restore_model(model_path, net)\n",
    "    return net\n",
    "\n",
    "def make_dataset(config, loader, extensions, model_path):\n",
    "    net = build_net(config, model_path)\n",
    "    path_feature = {}\n",
    "    dir = config[\"batches_dir\"]\n",
    "    dir_list = os.listdir(dir)\n",
    "    input_size = (config[\"img_w\"], config[\"img_h\"])\n",
    "    for target in sorted(dir_list):\n",
    "        # target is the class folder name (i.e. 999, 2, 1000)\n",
    "        d = os.path.join(dir, target)\n",
    "\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            images = []\n",
    "            \n",
    "            # first extract features\n",
    "            for fname in sorted(fnames):\n",
    "                if has_file_allowed_extension(fname, extensions):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    path_feature[path] = {}\n",
    "                    image = cv2.imread(path)\n",
    "                    image = cv2.resize(image, input_size)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)     \n",
    "                    images.append(image)\n",
    "\n",
    "            images = np.asarray(images, dtype=np.float32)         \n",
    "            images = ((images / 255.0) - [0.485, 0.456, 0.406]) \\\n",
    "                    / [0.229, 0.224, 0.225]\n",
    "            images = np.transpose(images, (0, 3, 1, 2))\n",
    "            images = images.astype(np.float32)\n",
    "            images = torch.from_numpy(images)\n",
    "\n",
    "            images = images.cuda()    \n",
    "            with torch.no_grad():\n",
    "                f = net(images)  \n",
    "                f = f.cpu().numpy()\n",
    "                f = f / np.linalg.norm(f, axis=1,keepdims=True)\n",
    "            \n",
    "            # make path_feature pair (features are not normalized)\n",
    "            for idx, fname in enumerate(sorted(fnames)):\n",
    "                if has_file_allowed_extension(fname, extensions):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    path_feature[path][\"feature\"] = f[idx]\n",
    "    net = None\n",
    "    return path_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the features and mask to build dataloader\n",
    "\n",
    "IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n",
    "\n",
    "## pcb_6_1536 model\n",
    "#model_path = \"/world/data-gpu-57/xulie/pytorch-output/market_pcb_1536_test_kaimin/model_best.pth\"\n",
    "#path_feature_file = \"/world/data-gpu-94/sysu-reid/market_dataset/path_feature_1536.pkl\"\n",
    "\n",
    "## 256_softmax_baseline\n",
    "#model_path = \"/world/data-gpu-57/xulie/pytorch-output/market_256_normal/model_best.pth\"\n",
    "#path_feature_file = \"/world/data-gpu-94/sysu-reid/market_dataset/path_feature_256.pkl\"\n",
    "\n",
    "## pcb_4_1024 \n",
    "model_path = \"/world/data-gpu-57/xulie/pytorch-output/market_pcb_4_1024_re/model_best.pth\"\n",
    "path_feature_file = \"/world/data-gpu-94/sysu-reid/market_dataset/path_feature_1024.pkl\"\n",
    "\n",
    "\n",
    "path_mask_file = \"/world/data-gpu-94/sysu-reid/market_dataset/path_mask.pkl\"\n",
    "\n",
    "if os.path.exists(path_feature_file):\n",
    "    with open(path_feature_file, 'r') as f:\n",
    "        path_feature = pickle.load(f)\n",
    "else:\n",
    "    print(\"generating data_set...\")\n",
    "    path_feature = make_dataset(config, default_loader, IMG_EXTENSIONS, model_path)\n",
    "    with open(path_feature_file, 'w') as f:\n",
    "        pickle.dump(path_feature, f)\n",
    "    print(\"feature saved!\")\n",
    "        \n",
    "with open(path_mask_file, 'r') as f:\n",
    "    path_mask = pickle.load(f)\n",
    "    \n",
    "\n",
    "# !!!!\n",
    "# this img size and batch_size is used for GAN, which is different from the reID model\n",
    "config[\"img_w\"] = 128\n",
    "config[\"img_h\"] = 256\n",
    "config[\"batch_size\"] = config[\"batch_sampling_params\"][\"batch_size\"] * len(config[\"parallels\"]) \n",
    "config[\"data_augmentation\"][\"imagenet_static\"] = True\n",
    "\n",
    "for key, value in path_feature.iteritems():\n",
    "    mask = np.array(cv2.resize(path_mask[key]['mask'], (config[\"img_w\"], config[\"img_h\"])), dtype=np.float32)\n",
    "    mask = np.expand_dims(mask, 0)\n",
    "    path_feature[key]['mask'] = mask\n",
    "    \n",
    "from input_pipeline.image_data_reader import init_data_loader \n",
    "data_loader = init_data_loader(config, 4, path_feature)\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(data_loader))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "print real_batch[0][:64].shape\n",
    "\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:64], nrow=16,padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[3][:64], nrow=16,padding=2, normalize=True).cpu(),(1,2,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_feature = {}\n",
    "\n",
    "for key, item in path_feature.iteritems():\n",
    "    label = key.split('/')[5]\n",
    "    if label not in label_feature.keys():\n",
    "        label_feature[label] = []\n",
    "    label_feature[label].append({'path': key, 'feature':item['feature'],'mask':item['mask']})\n",
    "\n",
    "fix_noise = []\n",
    "images = []\n",
    "img_count = 0\n",
    "\n",
    "for label in label_feature.keys():\n",
    "    for person in label_feature[label]:\n",
    "        img_count += 1\n",
    "        path = person[\"path\"]\n",
    "        fix_noise.append(person[\"feature\"])\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (config[\"img_w\"], config[\"img_h\"]))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)     \n",
    "        images.append(image)\n",
    "        if img_count % 8 == 0:\n",
    "            break\n",
    "    if img_count == 128:\n",
    "        break\n",
    "\n",
    "images = np.asarray(images, dtype=np.float32) \n",
    "print images.shape\n",
    "if config[\"data_augmentation\"][\"imagenet_static\"]:\n",
    "    images = ((images / 255.0) - [0.485, 0.456, 0.406]) \\\n",
    "                / [0.229, 0.224, 0.225]\n",
    "else:\n",
    "    images = ((images / 255.0) - [0.5, 0.5, 0.5]) \\\n",
    "                / [0.5, 0.5, 0.5]\n",
    "        \n",
    "images = np.transpose(images, (0, 3, 1, 2))\n",
    "images = images.astype(np.float32)\n",
    "images = torch.from_numpy(images)\n",
    "\n",
    "#fixed_noise = torch.randn(64, nz, device=device)\n",
    "#fixed_noise = real_batch[2].cuda()\n",
    "fixed_noise = torch.tensor(fix_noise)\n",
    "#noise_a = torch.randn(fixed_noise.shape[0], n_noise)\n",
    "#fixed_noise = torch.cat([fixed_noise, noise_a], 1).cuda()\n",
    "print fixed_noise.shape\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(images[:128], nrow=16, padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN training\n",
    "Please refer https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workers = 4\n",
    "batch_size = config[\"batch_size\"]\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = config[\"model_params\"][\"feature_dim\"]\n",
    "n_noise = 100\n",
    "# Size of feature maps in generator (depth)\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator (depth)\n",
    "ndf = 32\n",
    "\n",
    "num_epochs = 50\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "cls_net = None\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will load the REID model to do the classification\n",
    "# This cell can be skipped and not used\n",
    "\n",
    "cls_net = build_net(config, model_path, is_training=True)\n",
    "cls_net.cuda()\n",
    "cls_net = nn.DataParallel(cls_net)\n",
    "print cls_net\n",
    "#print cls_net(real_batch[0].cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "mean = torch.tensor([[[[0.485]], [[0.456]], [[0.406]]]])\n",
    "var = torch.tensor([[[[0.229]], [[0.224]], [[0.225]]]])\n",
    "kernel_size = (8,4) if config[\"img_h\"] == 256 else (4,4)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.re_norm_mean = torch.nn.Parameter(mean, requires_grad = False)\n",
    "        self.re_norm_var = torch.nn.Parameter(var, requires_grad = False)\n",
    "        self.register_buffer('m_const', self.re_norm_mean)\n",
    "        self.register_buffer('v_const', self.re_norm_var)\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, kernel_size, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            #nn.Sigmoid()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(-1).unsqueeze(-1)\n",
    "        if config[\"data_augmentation\"][\"imagenet_static\"]:\n",
    "            output = (torch.nn.functional.sigmoid(self.main(input)) - self.m_const) / self.v_const\n",
    "        else:\n",
    "            output = (torch.nn.functional.tanh(self.main(input)))\n",
    "        return output\n",
    "\n",
    "netG = Generator()\n",
    "netG = netG.cuda()\n",
    "netG = nn.DataParallel(netG)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "print (real_batch[2].shape)\n",
    "print netG(real_batch[2].cuda()).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            SpectralNorm(nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            SpectralNorm(nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            SpectralNorm(nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            SpectralNorm(nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            SpectralNorm(nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        )\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "        self.cls_2 = nn.Conv2d(ndf * 8, 1, kernel_size, 1, 0, bias=False)\n",
    "        self.fc = nn.Conv2d(ndf * 8, config[\"num_labels\"], kernel_size, 1, 0, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.main(input)\n",
    "        x_1 = self.cls_2(x)\n",
    "        x_1 = torch.nn.functional.sigmoid(x_1).view(-1)\n",
    "        x_2 = self.fc(x)\n",
    "        return x_1, x_2\n",
    "\n",
    "    \n",
    "netD = Discriminator().cuda()          \n",
    "netD = nn.DataParallel(netD)\n",
    "\n",
    "print (real_batch[0].shape)\n",
    "print (real_batch[3].shape)\n",
    "#real = torch.cat([real_batch[0], real_batch[3]], 1)\n",
    "#print (real_batch[0].shape)\n",
    "#print netD(real_batch[0].cuda())[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "softmax_loss = nn.CrossEntropyLoss()   \n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "#optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "optimizerD = torch.optim.Adam(filter(lambda p: p.requires_grad, netD.parameters()), lr=lr, betas=(beta1,0.999))\n",
    "optimizerG = torch.optim.Adam(filter(lambda p: p.requires_grad, netG.parameters()), lr=lr, betas=(beta1,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# savemodel\n",
    "def save_model():\n",
    "    model_save_path = \"mask_1024_noise\"\n",
    "    \n",
    "    netD.eval()\n",
    "    netG.eval()\n",
    "    \n",
    "    state_dict_g = netG.state_dict()\n",
    "    state_dict_d = netD.state_dict()\n",
    "\n",
    "    torch.save(state_dict_g, model_save_path + \"_g.pth\")\n",
    "    torch.save(state_dict_d, model_save_path + \"_d.pth\")\n",
    "    \n",
    "    netD.train()\n",
    "    netG.train()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def compute_batch_acc(outputs, labels, use_pcb=False):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    if use_pcb:\n",
    "        _, pred = torch.max(torch.mean(torch.stack(outputs), dim=0).data, 1)\n",
    "    else:\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "    batch_acc = torch.sum(pred == labels).item() * 1.0 / batch_size\n",
    "    return batch_acc\n",
    "\n",
    "def get_loss_d(img, label_cls, label_id, mask=None, cls_net=None):\n",
    "    if mask is not None:\n",
    "        img_m =  torch.cat([img, mask], 1)\n",
    "    else:\n",
    "        img_m = img\n",
    "        \n",
    "    b_size = img_m.shape[0]\n",
    "    output, pred_id = netD(img_m)\n",
    "    if cls_net is not None:\n",
    "        pred_id = cls_net(img)\n",
    "   \n",
    "    output = output.view(-1)\n",
    "    pred_id = pred_id.view(-1, config[\"num_labels\"])\n",
    "\n",
    "    # Calculate fake/real loss and acc\n",
    "    errD_cls = criterion(output, label_cls)\n",
    "    D_x = output.mean().item()\n",
    "    \n",
    "    # softmax loss and acc\n",
    "    errD_id = softmax_loss(pred_id, label_id)\n",
    "    accD = compute_batch_acc(pred_id, label_id) * 100.0\n",
    "\n",
    "    return errD_cls, errD_id, accD, D_x\n",
    "\n",
    "def get_loss_reid(img, label_id, cls_net, use_pcb):\n",
    "    pred_id_REID = cls_net(fake)\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    \n",
    "    if use_pcb is False:\n",
    "        errG_id_REID = softmax_loss(pred_id_REID, data[1]) \n",
    "    else:\n",
    "        errG_id_REID = sum([softmax_loss(output, data[1]) for output in pred_id_REID])\n",
    "    \n",
    "    accG_REID = compute_batch_acc(pred_id_REID, data[1], use_pcb=True) * 100\n",
    "    return errG_id_REID, accG_REID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop\n",
    "The L1 loss should be lower than 1.5(10x) to generate good-looking images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "save_path = \"img_list_test.pkl\"\n",
    "torch.cuda.empty_cache()\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "D_G_z2 = 0\n",
    "D_G_z1 = 0\n",
    "num_epochs = 50000\n",
    "err_l1 = 0\n",
    "accG_REID = 0\n",
    "print(\"Starting Training Loop...\")\n",
    "update_g = False\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        data = [a.cuda() for a in data]\n",
    "        # control the D's update frequency\n",
    "        update_d = (random.random() > 0.3)\n",
    "        \n",
    "        # Generate batch of latent vectors\n",
    "        #noise = (data[2] + torch.randn_like(data[2], device=device) * 0.01)\n",
    "        noise = torch.randn_like(data[2]).cuda()\n",
    "        #noise = torch.cat([data[2], torch.randn(data[2].shape[0],n_noise)], 1)\n",
    "        noise_norm = noise.pow(2).sum(1).pow(0.5).view(-1, 1)\n",
    "        noise = noise / noise_norm.view(-1, 1)\n",
    "        \n",
    "        b_size = data[0].size(0)\n",
    "        #noise = torch.randn(b_size, nz,  device=device)\n",
    "        label_cls = torch.full((b_size,), real_label, device=device)\n",
    "        \n",
    "        netD.zero_grad()     \n",
    "        errD_real_cls, errD_real_id, accD_real, D_x = get_loss_d(data[0], label_cls, data[1])\n",
    "        if update_d:\n",
    "            errD_real_cls.backward(retain_graph=True)\n",
    "            #errD_real_id.backward(retain_graph=True)\n",
    "        \n",
    "        ## Train with all-fake batch\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)      \n",
    "        label_cls.fill_(fake_label)\n",
    "        \n",
    "        errD_fake_cls, errD_fake_id, accD_fake, D_G_z1 = get_loss_d(fake.detach(), label_cls, data[1])\n",
    "        \n",
    "        if update_d:\n",
    "            errD_fake_cls.backward(retain_graph=True)\n",
    "\n",
    "        # Update D\n",
    "        if update_d:    \n",
    "            optimizerD.step()\n",
    "            \n",
    "        errD_cls = float(errD_fake_cls + errD_real_cls)\n",
    "        errD_id = float(errD_real_id + errD_fake_id)\n",
    "        accD = float((accD_fake + accD_real) / 2)\n",
    "\n",
    "            \n",
    "        ########################################################\n",
    "        netG.zero_grad()\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        label_cls.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        \n",
    "        fake_m = fake#torch.cat([fake, data[3]], 1)\n",
    "        output, pred_id = netD(fake_m)\n",
    "        \n",
    "        output = output.view(-1)\n",
    "        pred_id = pred_id.view(-1, config[\"num_labels\"])\n",
    "        \n",
    "        if cls_net is not None:\n",
    "            use_pcb = config[\"model_params\"].get(\"pcb_n_parts\", 0) > 0\n",
    "            #errG_id_REID, accG_REID = get_loss_reid(fake, data[1], cls_net, use_pcb)\n",
    "            #errG_id_REID.backward(retain_graph=True)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG_cls = criterion(output, label_cls)\n",
    "        errG_id = softmax_loss(pred_id, data[1]) \n",
    "        accG = compute_batch_acc(pred_id, data[1]) * 100\n",
    "        \n",
    "        err_l1 = l1_loss(fake * data[3], data[0] * data[3])\n",
    "        # Calculate gradients for G\n",
    "        errG_cls.backward(retain_graph=False)\n",
    "        #errG_id.backward()\n",
    "        \n",
    "        #err_l1.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "        #if D_G_z1 != D_G_z2:\n",
    "            print('[%d][%d/%d]\\tL_D: %.3f|%.3f  L_G: %.3f|%.3f D(x): %.3f|%2.1f  D(G(z)): %.3f/%.3f|%2.1f;%2.1f L1: %4f'\n",
    "                  % (epoch%100, i, len(data_loader),\n",
    "                     errD_cls, errD_real_id.item(), errG_cls.item(), errG_id, D_x, \n",
    "                     accD_real, D_G_z1, D_G_z2, accG, accG_REID, err_l1))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        #G_losses.append(errG_cls.item())\n",
    "        #D_losses.append(errD_cls)\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (i == len(data_loader)-1) and (epoch % 10 == 0 and epoch > 1):\n",
    "            with torch.no_grad():\n",
    "                save_model()\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake[:128],nrow=16, padding=2, normalize=True).numpy())\n",
    "            img_list = img_list[-3:]\n",
    "            with open(save_path, 'w') as f:\n",
    "                pickle.dump(img_list[-3:], f)\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results\n",
    "You can write this code in another file that you don't need to stop training to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.axis(\"off\")\n",
    "#plt.imshow(np.transpose(img_list[2],(1,2,0)), animated=True)\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list[-20:]]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# savemodel\n",
    "def save_model():\n",
    "    model_save_path = \"mask_plain\"\n",
    "    \n",
    "    netD.eval()\n",
    "    netG.eval()\n",
    "    \n",
    "    state_dict_g = netG.state_dict()\n",
    "    state_dict_d = netD.state_dict()\n",
    "\n",
    "    torch.save(state_dict_g, model_save_path + \"_g.pth\")\n",
    "    torch.save(state_dict_d, model_save_path + \"_d.pth\")\n",
    "    \n",
    "    #netD.cuda()\n",
    "    #netG.cuda()\n",
    "    netD.train()\n",
    "    netG.train()\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_save_path = \"mask_plain\"\n",
    "state_dict_d = torch.load(model_save_path + \"_d.pth\")\n",
    "state_dict_g = torch.load(model_save_path + \"_g.pth\")\n",
    "netD = Discriminator()\n",
    "netG = Generator()\n",
    "netD = nn.DataParallel(netD)\n",
    "netG = nn.DataParallel(netG)\n",
    "\n",
    "netD.load_state_dict(state_dict_d, strict=True)\n",
    "netG.load_state_dict(state_dict_g, strict=True)\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "netD.train()\n",
    "netG.train()\n",
    "\n",
    "optimizerD = torch.optim.Adam(filter(lambda p: p.requires_grad, netD.parameters()), lr=lr, betas=(beta1,0.999))\n",
    "optimizerG = torch.optim.Adam(filter(lambda p: p.requires_grad, netG.parameters()), lr=lr, betas=(beta1,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

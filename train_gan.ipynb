{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "import cv2\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from IPython.display import HTML\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from nets.model_main import ft_net\n",
    "from utils import model_utils\n",
    "\n",
    "\n",
    "config = json.load(open(\"params.json\", \"r\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"parallels\"])[1:-1]\n",
    "print \"Currently using GPU\", str(config[\"parallels\"])[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpectralNorm\n",
    "https://github.com/christiancosgrove/pytorch-spectral-normalization-gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataloader for GAN\n",
    "- Every data iteration return in the format:  **{img, label, feature, mask}**\n",
    "\n",
    "- You need to extract the feature with provided model (for further usage, the features will be saved!)\n",
    "\n",
    "- You need to provide the file containing all the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic info \n",
    "\n",
    "config[\"batches_dir\"] = \"/world/data-gpu-94/sysu-reid/market_dataset\"\n",
    "label_dirs = [p for p in os.listdir(config[\"batches_dir\"])\n",
    "              if os.path.isdir(os.path.join(config[\"batches_dir\"], p))]\n",
    "config[\"num_labels\"] = len(label_dirs)\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "    \n",
    "def has_file_allowed_extension(filename, extensions):\n",
    "    \"\"\"Checks if a file is an allowed extension.\n",
    "\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(ext) for ext in extensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtions to build pytorch network and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_net(config, model_path):\n",
    "    net = ft_net(config,\n",
    "                 model_name=config[\"model_params\"][\"model\"],\n",
    "                 feature_dim=config[\"model_params\"].get(\"feature_dim\", 256),\n",
    "                 pcb_n_parts=config[\"model_params\"].get(\"pcb_n_parts\", 0),\n",
    "                 is_training=False)\n",
    "    net.train(False)\n",
    "    net = net.cuda()\n",
    "    net.eval()  \n",
    "    \n",
    "    # Restore pretrain model\n",
    "    model_utils.restore_model(model_path, net)\n",
    "    return net\n",
    "\n",
    "def make_dataset(config, loader, extensions, model_path):\n",
    "    net = build_net(config, model_path)\n",
    "    path_feature = {}\n",
    "    dir = config[\"batches_dir\"]\n",
    "    dir_list = os.listdir(dir)\n",
    "    input_size = (config[\"img_w\"], config[\"img_h\"])\n",
    "    for target in sorted(dir_list):\n",
    "        # target is the class folder name (i.e. 999, 2, 1000)\n",
    "        d = os.path.join(dir, target)\n",
    "\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            images = []\n",
    "            \n",
    "            # first extract features\n",
    "            for fname in sorted(fnames):\n",
    "                if has_file_allowed_extension(fname, extensions):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    path_feature[path] = {}\n",
    "                    image = cv2.imread(path)\n",
    "                    image = cv2.resize(image, input_size)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)     \n",
    "                    images.append(image)\n",
    "\n",
    "            images = np.asarray(images, dtype=np.float32)         \n",
    "            images = ((images / 255.0) - [0.485, 0.456, 0.406]) \\\n",
    "                    / [0.229, 0.224, 0.225]\n",
    "            images = np.transpose(images, (0, 3, 1, 2))\n",
    "            images = images.astype(np.float32)\n",
    "            images = torch.from_numpy(images)\n",
    "\n",
    "            images = images.cuda()    \n",
    "            with torch.no_grad():\n",
    "                f = net(images)  \n",
    "                f = f.cpu().numpy()\n",
    "                f = f / np.linalg.norm(f, axis=1,keepdims=True)\n",
    "            \n",
    "            # make path_feature pair (features are not normalized)\n",
    "            for idx, fname in enumerate(sorted(fnames)):\n",
    "                if has_file_allowed_extension(fname, extensions):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    path_feature[path][\"feature\"] = f[idx]\n",
    "    net = None\n",
    "    return path_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the features and mask to build dataloader\n",
    "\n",
    "IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n",
    "\n",
    "model_path = \"/world/data-gpu-57/xulie/pytorch-output/market_pcb_1536_test_kaimin/model_best.pth\"\n",
    "path_feature_file = \"/world/data-gpu-94/sysu-reid/market_dataset/path_feature_1536.pkl\"\n",
    "path_mask_file = \"/world/data-gpu-94/sysu-reid/market_dataset/path_mask.pkl\"\n",
    "\n",
    "if os.path.exists(path_feature_file):\n",
    "    with open(path_feature_file, 'r') as f:\n",
    "        path_feature = pickle.load(f)\n",
    "else:\n",
    "    path_feature = make_dataset(config, default_loader, IMG_EXTENSIONS, model_path)\n",
    "    with open(path_feature_file, 'w') as f:\n",
    "        pickle.dump(path_feature, f)\n",
    "        \n",
    "with open(path_mask_file, 'r') as f:\n",
    "    path_mask = pickle.load(f)\n",
    "    \n",
    "\n",
    "# !!!!\n",
    "# this img size and batch_size is used for GAN, which is different from the reID model\n",
    "config[\"img_w\"] = 128\n",
    "config[\"img_h\"] = 128\n",
    "config[\"batch_size\"] = config[\"batch_sampling_params\"][\"batch_size\"] * len(config[\"parallels\"]) \n",
    "\n",
    "for key, value in path_feature.iteritems():\n",
    "    mask = np.array(cv2.resize(path_mask[key]['mask'], (config[\"img_w\"], config[\"img_h\"])), dtype=np.float32)\n",
    "    mask = np.expand_dims(mask, 0)\n",
    "    path_feature[key]['mask'] = mask\n",
    "    \n",
    "from input_pipeline.image_data_reader import init_data_loader \n",
    "data_loader = init_data_loader(config, 4, path_feature)\n",
    "\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(data_loader))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[3][:64], padding=2, normalize=True).cpu(),(1,2,0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN training\n",
    "Please refer https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 4\n",
    "batch_size = config[\"batch_size\"]\n",
    "image_size = config[\"img_w\"]\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = config[\"model_params\"][\"feature_dim\"]\n",
    "\n",
    "# Size of feature maps in generator (depth)\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator (depth)\n",
    "ndf = 32\n",
    "\n",
    "num_epochs = 50\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(-1).unsqueeze(-1)\n",
    "        return self.main(input)\n",
    "\n",
    "netG = Generator()\n",
    "netG = netG.cuda()\n",
    "netG = nn.DataParallel(netG)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "print (real_batch[2].shape)\n",
    "print netG(real_batch[2].cuda()).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            SpectralNorm(nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            SpectralNorm(nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 32 x 32\n",
    "            SpectralNorm(nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            SpectralNorm(nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            SpectralNorm(nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "    \n",
    "netD = Discriminator().cuda()          \n",
    "netD = nn.DataParallel(netD)\n",
    "\n",
    "print (real_batch[0].shape)\n",
    "print (real_batch[3].shape)\n",
    "real = torch.cat([real_batch[0], real_batch[3]], 1)\n",
    "print (real.shape)\n",
    "print netD(real.cuda()).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "#fixed_noise = torch.randn(64, nz, device=device)\n",
    "fixed_noise = real_batch[2].cuda()\n",
    "print fixed_noise.shape\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "#optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerD = torch.optim.Adam(filter(lambda p: p.requires_grad, netD.parameters()), lr=lr, betas=(beta1,0.999))\n",
    "optimizerG = torch.optim.Adam(filter(lambda p: p.requires_grad, netG.parameters()), lr=lr, betas=(beta1,0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop\n",
    "The L1 loss should be lower than 1.5(10x) to generate good-looking images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "save_path = \"img_list_test.pkl\"\n",
    "torch.cuda.empty_cache()\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "D_G_z2 = 0\n",
    "D_G_z1 = 0\n",
    "epoch = 0\n",
    "err_l1 = 0\n",
    "print(\"Starting Training Loop...\")\n",
    "update_g = False\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(1000):\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        # control the D's update frequency\n",
    "        update_d = (random.random() > 0.0)\n",
    "        \n",
    "        # Generate batch of latent vectors\n",
    "        noise = (data[2] + torch.randn_like(data[2]) * 0.01).cuda()\n",
    "        \n",
    "        # concate mask with training imgs\n",
    "        real_cpu = data[0].cuda()\n",
    "        real_cpu_m = torch.cat([data[0].cuda(), data[3].cuda()], 1)\n",
    "\n",
    "        b_size = real_cpu_m.size(0)\n",
    "        #noise = torch.randn(b_size, nz,  device=device)\n",
    "        label = torch.full((b_size,), real_label).cuda()\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu_m).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        D_x = output.mean().item()\n",
    "        if update_d:\n",
    "            errD_real.backward()\n",
    "        \n",
    "        ## Train with all-fake batch\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)      \n",
    "        fake_m = torch.cat([fake.cuda(), data[3].cuda()], 1)\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake_m.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD = errD_real + errD_fake\n",
    "        D_G_z1 = output.mean().item()\n",
    "        if update_d:\n",
    "            errD_fake.backward()\n",
    "        # Update D\n",
    "        if update_d:    \n",
    "            optimizerD.step()\n",
    "\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake_m).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        err_l1 = l1_loss(fake * data[3].cuda(), real_cpu * data[3].cuda()) * 10\n",
    "        # Calculate gradients for G\n",
    "        errG.backward(retain_graph=True)\n",
    "        \n",
    "        #err_l1.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 30 == 0:\n",
    "        #if D_G_z1 != D_G_z2:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f L1: %4f'\n",
    "                  % (epoch, num_epochs, i, len(data_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2, err_l1))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (i == len(data_loader)-1) or (iters % 200 == 0 and iters > 0):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True).numpy())\n",
    "            img_list = img_list[-3:]\n",
    "            with open(save_path, 'w') as f:\n",
    "                pickle.dump(img_list[-3:], f)\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results\n",
    "You can write this code in another file that you don't need to stop training to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.axis(\"off\")\n",
    "#plt.imshow(np.transpose(img_list[2],(1,2,0)), animated=True)\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list[-20:]]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# savemodel\n",
    "save_path = \"256_1536_with_mask\"\n",
    "state_dict_g = netG.state_dict()\n",
    "state_dict_d = netD.state_dict()\n",
    "\n",
    "netD.eval()\n",
    "netG.eval()\n",
    "\n",
    "torch.save(state_dict_g, save_path + \"_g.pth\")\n",
    "torch.save(state_dict_d, save_path + \"_d.pth\")\n",
    "\n",
    "net.train()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
